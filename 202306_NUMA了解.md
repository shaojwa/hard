### NUMA架构对内存的影响
原本的X86结构，CPU和内存通过北桥连接，北桥中有内存控制器芯片（MCH=Memory Controller Hub）。
但是因为CPU的变多，前端总线FSB成了CPU和内存控制器之间的性能瓶颈。于是在AMD引入64位x86结构时，实现NUMA架构。
然后Intel也对出NUMA类型的Nehalem架构。

NUMA架构把北桥上的内存控制器放到了CPU内部。多个CPU之间，通过QPI连接。

#### NUMA本地的资源
主要是 内存，PCIe总线，ACPI规范一般是这么抽象定义一个NUMA节点的。

#### Uncore
是NUMA节点内，除去Core的部分，一般有iMC, PCIe root complex, QPI控制器，L3缓存，CBox，以及其他外设控制器。

#### ccNUMA
cache coherent NUMA，缓存连贯的NUMA。

####　访问延时
在Intel lvyBridge的NUMA平台上测试，远程内存访问是本地内存的2倍。

####　本地IO资源
PCIe之外，也有将IB总线继承到CPU里的。这样IB设备也属于NUMA Node的一部分。

####　NUMA亲和性
有CPU NUMA的亲和性，就是站在CPU的角度看，哪些内存访问块，有更低的时延。
有设备的NUMA亲和性，就是指从PCIe外设的角度看，这个设备的IO，把数据放入什么内存速度更快。
一般都是这个IO设备所在NUMA节点所在的内存，另外一个就是，这个设备产生的中断之后，由什么CPU处理。
当然最好是内存所在NUMA节点的CPU处理会更块。

#### 硬件的NUMA层级结构
